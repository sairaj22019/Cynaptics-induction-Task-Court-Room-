{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IkwHUGVVn1x1",
        "outputId": "9dd37de6-177f-4cc7-864e-50fa47b141fe"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "from huggingface_hub import login\n",
        "load_dotenv() \n",
        "token = os.getenv(\"HF_API_TOKEN\")\n",
        "login(token=token)\n",
        "model=\"tiiuae/falcon-7b-instruct\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Two–Lawyer Agents (Defense & Prosecution)\n",
        "from __future__ import annotations\n",
        "\n",
        "from typing import List, Dict\n",
        "from huggingface_hub import InferenceClient\n",
        "\n",
        "class LawyerAgent:\n",
        "    def __init__(self,\n",
        "                 name: str,\n",
        "                 system_prompt: str,\n",
        "                 model: str = model):\n",
        "        self.name = name\n",
        "        self.system_prompt = system_prompt.strip()\n",
        "        self.history: List[Dict[str, str]] = []      # list of {\"role\": ..., \"content\": ...}\n",
        "        self.client = InferenceClient(\n",
        "            model,\n",
        "            token=token          # make sure this env‑var is set\n",
        "        )\n",
        "\n",
        "    # ---- helper for HF prompt formatting ----------\n",
        "    def _format_prompt(self, user_msg: str) -> str:\n",
        "        \"\"\"\n",
        "        Formats a full prompt that includes\n",
        "        * system prompt\n",
        "        * prior turns\n",
        "        * new user message\n",
        "        \"\"\"\n",
        "        messages = [{\"role\": \"system\", \"content\": self.system_prompt}]\n",
        "        messages.extend(self.history)\n",
        "        messages.append({\"role\": \"user\", \"content\": user_msg})\n",
        "\n",
        "        # HF text-generation endpoints expect a single string.\n",
        "\n",
        "        prompt = \"\"\n",
        "        for m in messages:\n",
        "            prompt += f\"<|{m['role']}|>\\n{m['content']}\\n\"\n",
        "        prompt += \"<|assistant|>\\n\"\n",
        "        return prompt\n",
        "\n",
        "    # ---- produce a reply --------------------------\n",
        "    def respond(self, user_msg: str, **gen_kwargs) -> str:\n",
        "        prompt = self._format_prompt(user_msg)\n",
        "        completion = self.client.text_generation(\n",
        "            prompt,\n",
        "            max_new_tokens=512,\n",
        "            temperature=0.7,\n",
        "            do_sample=True,\n",
        "            stream=False,\n",
        "            **gen_kwargs\n",
        "        )\n",
        "        answer = completion.strip()\n",
        "        # keep chat memory\n",
        "        self.history.append({\"role\": \"user\", \"content\": user_msg})\n",
        "        self.history.append({\"role\": \"assistant\", \"content\": answer})\n",
        "        return answer\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "class MainAgent:\n",
        "    def __init__(self, name: str, system_prompt: str, model: str, token: str):\n",
        "        self.name = name\n",
        "        self.system_prompt = system_prompt.strip()\n",
        "        self.history: List[Dict[str, str]] = []\n",
        "        self.client = InferenceClient(model, token=token)\n",
        "\n",
        "    def _format_prompt(self, user_msg: str) -> str:\n",
        "        messages = [{\"role\": \"system\", \"content\": self.system_prompt}]\n",
        "        messages.extend(self.history)\n",
        "        messages.append({\"role\": \"user\", \"content\": user_msg})\n",
        "        prompt = \"\"\n",
        "        for m in messages:\n",
        "            prompt += f\"<|{m['role']}|>\\n{m['content']}\\n\"\n",
        "        prompt += \"<|assistant|>\\n\"\n",
        "        return prompt\n",
        "\n",
        "    def respond(self, user_msg: str, **gen_kwargs) -> str:\n",
        "        prompt = self._format_prompt(user_msg)\n",
        "        completion = self.client.text_generation(\n",
        "            prompt,\n",
        "            max_new_tokens=512,\n",
        "            temperature=0.7,\n",
        "            do_sample=True,\n",
        "            stream=False,\n",
        "            **gen_kwargs\n",
        "        )\n",
        "        answer = completion.strip()\n",
        "        self.history.append({\"role\": \"user\", \"content\": user_msg})\n",
        "        self.history.append({\"role\": \"assistant\", \"content\": answer})\n",
        "        return answer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Defendent\n",
        "class DefendantAgent(MainAgent):\n",
        "    def __init__(self, model: str, token: str , system_prompt: str ):\n",
        "        \n",
        "        super().__init__(\"Defendant\", system_prompt, model, token)\n",
        "    def testify(self, question: str) -> str:\n",
        "        return self.respond(f\"As the defendant, answer: {question}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plaintiff\n",
        "class PlaintiffAgent(MainAgent):\n",
        "    def __init__(self, model: str, token: str , system_prompt: str):\n",
        "        \n",
        "        super().__init__(\"Plaintiff\", system_prompt, model, token)\n",
        "    def statement(self) -> str:\n",
        "        return self.respond(\"State your grievance and what you want from the court.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Judge\n",
        "class JudgeAgent(MainAgent):\n",
        "    def __init__(self, model: str, token: str , system_prompt: str):\n",
        "        \n",
        "        super().__init__(\"Judge\", system_prompt, model, token)\n",
        "    def verdict(self) -> str:\n",
        "        return self.respond(\"Deliver your verdict and explain your reasoning.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Eye witness\n",
        "class EyewitnessAgent(MainAgent):\n",
        "    def __init__(self, name: str, witnessed_details: str, model: str, token: str):\n",
        "        system_prompt = f\"\"\"\n",
        "        You are an eyewitness named {name}. You witnessed the following: {witnessed_details}\n",
        "        \n",
        "        Your role is to:\n",
        "        • Testify truthfully about what you saw, heard, or otherwise perceived\n",
        "        • Answer questions from both lawyers clearly and accurately\n",
        "        • Only speak about what you personally witnessed\n",
        "        • Admit when you are uncertain or don't remember details\n",
        "        \n",
        "        Style:\n",
        "        • Speak in first person (\"I saw...\")\n",
        "        • Use descriptive but factual language\n",
        "        • Maintain consistency in your account\n",
        "        • Show appropriate emotional reactions based on what you witnessed\n",
        "        \"\"\"\n",
        "        super().__init__(name, system_prompt, model, token)\n",
        "    \n",
        "    def testify(self) -> str:\n",
        "        \"\"\"Provide testimony about what you witnessed\"\"\"\n",
        "        return self.respond(\"Describe in detail what you witnessed regarding this case.\")\n",
        "    \n",
        "    def answer_prosecution(self, question: str) -> str:\n",
        "        \"\"\"Answer a question from the prosecution\"\"\"\n",
        "        return self.respond(f\"The prosecution asks: {question}\")\n",
        "    \n",
        "    def answer_defense(self, question: str) -> str:\n",
        "        \"\"\"Answer a question from the defense (cross-examination)\"\"\"\n",
        "        return self.respond(f\"The defense attorney asks during cross-examination: {question}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "# All prompts\n",
        "\n",
        "DEFENSE_SYSTEM = \"\"\"\n",
        "You are **Alex Carter**, lead *defense counsel*.\n",
        "Goals:\n",
        "• Protect the constitutional rights of the defendant.\n",
        "• Raise reasonable doubt by pointing out missing evidence or alternative explanations.\n",
        "• Be respectful to the Court and to opposing counsel.\n",
        "Style:\n",
        "• Crisp, persuasive, grounded in precedent and facts provided.\n",
        "• When citing precedent: give short case name + year (e.g., *Miranda v. Arizona* (1966)).\n",
        "Ethics:\n",
        "• Do not fabricate evidence; admit uncertainty when required.\n",
        "\"\"\"\n",
        "\n",
        "PROSECUTION_SYSTEM = \"\"\"\n",
        "You are **Jordan Blake**, *Assistant District Attorney* for the State.\n",
        "Goals:\n",
        "• Present the strongest good-faith case against the accused.\n",
        "• Lay out facts logically, citing exhibits or witness statements when available.\n",
        "• Anticipate and rebut common defense arguments.\n",
        "Style:\n",
        "• Formal but plain English; persuasive, with confident tone.\n",
        "Ethics:\n",
        "• Duty is to justice, not merely to win. Concede points when ethically required.\n",
        "\"\"\"\n",
        "\n",
        "Defendent_prompt = \"\"\"\n",
        "        You are the defendant. Respond truthfully to all questions about your actions and intentions.\n",
        "        Only speak when addressed. Admit what you know and clarify misunderstandings.\n",
        "        \"\"\"\n",
        "\n",
        "Plantiff_prompt = \"\"\"\n",
        "        You are the plaintiff. Clearly explain your grievance, the harm suffered, and what you seek from the court.\n",
        "        Answer questions honestly and provide details about the incident.\n",
        "        \"\"\"\n",
        "\n",
        "Judge_prompt = \"\"\"\n",
        "        You are the judge. Ensure fairness, rule on objections, explain the law, and make the final decision.\n",
        "        Remain impartial and base rulings on evidence and law.\n",
        "        \"\"\"\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "def run_trial(case_background: str, eyewitness_details: str = None):\n",
        "    \"\"\"\n",
        "    Simulates a complete trial using your defined agents.\n",
        "    \n",
        "    Args:\n",
        "        case_background (str): Factual summary of the case\n",
        "        eyewitness_details (str, optional): If provided, activates eyewitness testimony\n",
        "    \"\"\"\n",
        "    # Initialize core agents\n",
        "    defense = LawyerAgent(\"Defense\", DEFENSE_SYSTEM)\n",
        "    prosecution = LawyerAgent(\"Prosecution\", PROSECUTION_SYSTEM)\n",
        "    defendant = DefendantAgent(model=model, token=token,system_prompt=Defendent_prompt)\n",
        "    plaintiff = PlaintiffAgent(model=model, token=token,system_prompt=Plantiff_prompt)\n",
        "    judge = JudgeAgent(model=model, token=token,system_prompt=Judge_prompt)\n",
        "    \n",
        "    # Conditionally create eyewitness\n",
        "    eyewitness = None\n",
        "    if eyewitness_details:\n",
        "        eyewitness = EyewitnessAgent(\n",
        "            name=\"Eyewitness\",\n",
        "            witnessed_details=eyewitness_details,\n",
        "            model=\"microsoft/Phi-3-mini-4k-instruct\",\n",
        "            token=token\n",
        "        )\n",
        "    \n",
        "    # 1. Opening Statements\n",
        "    print(\"\\n=== OPENING STATEMENTS ===\")\n",
        "    print(\"PROSECUTION:\", prosecution.respond(f\"Present opening statement. Case details: {case_background}\"))\n",
        "    print(\"\\nDEFENSE:\", defense.respond(\"Present opening statement in response to prosecution\"))\n",
        "    \n",
        "    # 2. Plaintiff's Case\n",
        "    print(\"\\n=== PLAINTIFF TESTIMONY ===\")\n",
        "    print(\"PLAINTIFF:\", plaintiff.statement())\n",
        "    print(\"\\nPROSECUTION DIRECT:\", prosecution.respond(\"Ask follow-up questions to plaintiff\"))\n",
        "    print(\"\\nDEFENSE CROSS:\", defense.respond(\"Cross-examine the plaintiff\"))\n",
        "    \n",
        "    # 3. Eyewitness Testimony (if present)\n",
        "    if eyewitness:\n",
        "        print(\"\\n=== EYEWITNESS TESTIMONY ===\")\n",
        "        print(\"EYEWITNESS:\", eyewitness.testify())\n",
        "        print(\"\\nPROSECUTION DIRECT:\", prosecution.respond(\"Ask questions to eyewitness\"))\n",
        "        print(\"\\nDEFENSE CROSS:\", defense.respond(\"Cross-examine the eyewitness\"))\n",
        "    \n",
        "    # 4. Defendant's Case\n",
        "    print(\"\\n=== DEFENDANT TESTIMONY ===\")\n",
        "    print(\"DEFENSE DIRECT:\", defense.respond(\"Ask defendant to explain their side\"))\n",
        "    print(\"\\nDEFENDANT:\", defendant.testify(\"Explain your version of events\"))\n",
        "    print(\"\\nPROSECUTION CROSS:\", prosecution.respond(\"Cross-examine the defendant\"))\n",
        "    \n",
        "    # 5. Closing Arguments\n",
        "    print(\"\\n=== CLOSING ARGUMENTS ===\")\n",
        "    print(\"PROSECUTION:\", prosecution.respond(\"Present closing argument summarizing the case\"))\n",
        "    print(\"\\nDEFENSE:\", defense.respond(\"Present closing argument rebutting prosecution\"))\n",
        "    \n",
        "    # 6. Verdict\n",
        "    print(\"\\n=== JUDGE'S VERDICT ===\")\n",
        "    print(\"JUDGE:\", judge.verdict())\n",
        "    \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "case_background = (\n",
        "    \"The State alleges that John Doe stole proprietary algorithms from his former employer \"\n",
        "    \"and used them at a competitor. The charge is felony theft of trade secrets. \"\n",
        "    \"No physical evidence shows direct copying, but server logs indicate large downloads \"\n",
        "    \"two days before Doe resigned.\"\n",
        ")\n",
        "eyewitness_details=(\n",
        "    \"I noticed unusual activity on the server two days before John Doe resigned.\"\n",
        "    \"His account accessed and downloaded several proprietary algorithm files.\"\n",
        "    \"He wasn't authorized to work with those files at that time.\"\n",
        "    \"I reported the incident to my supervisor immediately.\"\n",
        ")\n",
        "run_trial(case_background,eyewitness_details)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
